{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Report on “Generative Adversarial Nets”\n",
    "    RuiZhe Wang 13163617"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords: \n",
    "generative model estimation procedure, framework, \n",
    "backpropagation, dropout algorithms, Markov chains, optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "This paper was written in 2014, and the author talks about a new framework which via an adversarial process to estimating generative models. In this process, the framework provides some training algorithms for model and many kinds of algorithm which do not need to use approximate inference or Markov chains. It is based on the assumption that the author has a good understanding of deep learning area and has basic knowledge of AI applications.\n",
    "\n",
    "In the big background of artificial intelligence, scholars found that deep learning can express the probability distribution of various data, and the most striking successes in deep learning have involved discriminative models. Lan (2013) talks about the “According to the dropout and backpropagation algorithms, these (discriminative models) which have a good gradient linear unit.” The author of the present intends to provide a completely new method and generative model estimation procedure could avoid these difficulties.\n",
    "\n",
    "In the \"references\" paragraph of this article, we find that the literature cited by the author was all published after 2006. We can conclude that evidence of a literature review is relevant and recent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content:\n",
    "The deep learning is an important branch of way to show the kinds of dataset met in artiﬁcial intelligence area which can find or create hierarchical models, such as natural languages symbols and natural images. “In fact, the \"deep\" in \"deep learning\" means the number of transformer data through layers and each level of layers try to change the input data messages to a hardly composite and abstract representation of expressive techniques.” (Hinton,2012)\n",
    "\n",
    "So far, the discriminative model is the most useful method in the deep learning area. Goodfellow and Bengio. Y (2013) showed that the deep-learning neural networks to research in parameterizing these models and it made complex and high-dimensional datasets, media models and text to recent advances by the progress in optimization methods processing. Because of the \"the arising of intractable probabilistic difficulty computations happened in the maximum likelihood estimation \", the less of hazard impact happed in the generative models.\n",
    "\n",
    "However, it still needs a lot of programming handwork to implement this model. And the author proposed a new generative model program that it can sidestep and research these difﬁculties. In this new framework, the author train two models stood against each other: the first one is the generative model G, it can get the dataset distribution; another one is the discriminative model D that can estimates data from training data. The model G can compare the counterfeiters and model D looks like the police: the former for use fake without detection, the latter to detect it. Both sides have come to a deadlock and keep improve their methods until the counterfeit looks just like the original. In the end, the framework and models can provide machine-learning specific training algorithms for lots of learning model and optimization algorithm.\n",
    "\n",
    "In this article, the author called \" generative model create samples model when input random noise information by through a multilayer perceptron\" as \"adversarial nets\" and list it as a special case. To use this example, the author only trains these two models using dropout and backpropagation models and using only forward propagation from the generative model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Innovation:\n",
    "This article provides a new method to generate the model of the framework, that is a new methodology. The author uses many formulas and self-made models to help prove his point of view，such as D and G control the two-player minimax following game with V(G, D) value function and algorithm process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![01.PNG](./01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Figure 1 is an explanation of the approach, a less informal but clear and unambiguous. If the reader wants to understand the Pg (generator’s distribution) over data x, the author put the Pz(Z) as noise variables, a mapping from these datasets to the data space called G(z; θg), where G is the multilayer perceptron function represented with parameters θg. On the other hand, the author defines a perceptron D(x; θd) that means outputs a single scalar and the D(x) represents the probability that information came from the data. At last, we can find the author train D to make the most of probability to most of the training examples about the G and train G to minimize log(1 − D(G(z))).\n",
    "\n",
    "At the next step, the author indicates the limitation of this equation: it might can no provide enough gradient for G to learn well because of when G is lower, D is excusably different from training data, so they can reject samples. In addition to enumerating the algorithms, the author also created model pictures:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![02.PNG](./02.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. D is a partially accurate classifier and Pg is similar to Pdata, it is an adversarial pair which near the convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.![11.PNG](./11.PNG)\n",
    "This formula from the data when algorithm D training discriminate samples in the inner loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. When D’s gradient has led the G(z) move to areas that when G updated data are more likely to be classified as data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. If pg = Pdata, G and D will have enough capacity to improve a level which both cannot improve. On the other hand, the discriminator cannot differentiate these two distributions and models,![05.PNG](./05.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four model pictures represent the process when the author trained the discriminative distribution by using the simultaneously update. D, blue and dashed line，means discriminative distribution. The data generating distribution px was composed of the dotted and black line, the pg (generative distribution consist of the solid and green line) will be updated when updating the D information. One part of the X area is above the horizontal line and the lower horizontal line which is the z sampled domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical quality:\n",
    "\n",
    "The quality of this published paper is very high, but somewhere need to be improved. First of all, the quality of the paper depends on the research messages and the author give very clearly described for research. It results that someone could test the experiment by themselves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![06.PNG](./06.PNG)\n",
    "                                           Picture 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picture 1 shows the processing through author give a hyperparameter K to test an algorithm. They input k= 1 and choose a method to minimize K consumption for their experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![07.PNG](./07.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picture 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These algorithms based on the momentum criterion, and author used it in experiments show. On the other hand, Rifai (2012) talk about the updates based on the gradient algorithm and models can apply by any learning rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"experiment\" part of the paper, the author also gives very detailed information. Depend on the series of training datasets which including the Toronto Face Database, MNIST and CIFAR-10(11). Bengio (2013) noted that rectifier Linear and sigmoid activations used in the generator nets and discriminator nets used to max out activations. As for Droupot (a kind of neural networks) (7), it applied in training the discriminator net but not applied at the generator's intermediate layers. In this experiment, only used noise as the input data. (9)\n",
    "\n",
    "The author training the dataset under Pg by using the Gaussian Parzen window to the G generated samples, and obtain by cross-validation at validation set can get the σ parameter of the Gaussians. The results are show in Table 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![08.PNG](./08.PNG) Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Table 1 picture based on Parzen log-likelihood estimates. As for MNIST, author compared the real-valued version against other datasets models. In the reported numbers of MNIST listing are meant log-likelihood test samples and with the calculation error examples. At the TFD listing, author used the information to calculate all dataset's folds of error. On the other hand, this picture shows each fold in the validation set with a different σ and using σ to cross-validated each training fold and computed all fold of mean log-likelihood. (2)\n",
    "\n",
    "In Figures 1 and Figures 2, we can see the author give the detail about the samples were extracted from generator sub-network after train. In their opinion, these samples will compare with other virtual generative models in lots of academic articles and literature in the literature and a good adversarial framework of immense potency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![09.PNG](./09.PNG) \n",
    "Figure 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MINST Example/\n",
    "B TFD Example/\n",
    "C CIFAR-10 (Connected Model)/\n",
    "D CIFAR-10 (Deconvolutional Generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![10.PNG](./10.PNG) Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 is the visualization of model samples. The rightmost yellow frames column give the example of the sample colouration in the nearest training. Because of the fair random draw samples without cherry-picked and the model has not memorized about the training set. Here the author proves that these three images give lots of details and real samples from the simulation model distributions without the hidden units which used the conditional means given samples. On the other hands, basic of the Markov chain does not use in the process so these samples are uncorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2: Lan (2013) showed the digits obtained by linearly interpolating between coordinates in z space of the full model. [4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X-factor and Application:\n",
    "\n",
    "As for my opinion, this technique in an application domain is for well-informed discussion and could apply to other research. This paper reflected the author's opinions about application domain to other research: \n",
    "1.If given X to help predict Z by training and testing auxiliary network, the learned approximately inference could be performed. Lan (2013) said that is particularly similar by using the wake-sleep algorithm and model simulation to the distributed inference net trained, but it has lots of good advantage. One is the inference could train fixed generator net but it doesn't need the finished training.\n",
    "2.Could use adversarial nets to implement a stochastic extension of the deterministic MP-DBM [11]. \n",
    "3.Efficiency improvements and Semi-supervised learning\n",
    "As for my opinion, this article has shown the feasibility and accuracy of the adversarial modelling relationship framework and make this research useful to use. In fact, I think this paper main contents could make a great and full discussion in the student group and class because this is a very useful research for antagonistic network frameworks and models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation:\n",
    "\n",
    "The quality of this paper is very high and it can be said to be a very classic paper. We can see this paper is tightly written from five points:\n",
    "1.\tThe arguments are founded on facts and compelling conviction. It's stimulating to depend on rigorous tests. Very many objective functions and confrontational network tests are used to prove their point of view.\n",
    "2.\tThe question is purely academic, and author have academic logics.\n",
    "3.\tWhat the article wants to express is very clear: Use models and sub-network to create images. Some forgery images have little difference with genuine images.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links: https://github.com/RuiZheWang99/31005-Ass1\n",
    "Generative Adversarial Nets: https://arxiv.org/pdf/1406.2661.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "[1] Bengio, Y. and Rifai, 2013, Better mixing via deep representations, In ICML’13.\n",
    "[2] Bengio, Y. and Yosinski, J, 2014, Deep generative stochastic networks trainable by backprop, In ICML’14.\n",
    "[3] Hinton and Neal, R. M. ,1995, The wake-sleep algorithm for unsupervised neural networks, pp1558–1559\n",
    "[4] Hinton and Salakhutdinov, ,2012, Improving neural networks by preventing co-adaptation of feature detectors. \n",
    "[5] Goodfellow, and Bengio. Y,2013, Multi-prediction deep Boltzmann machines\n",
    "[6] Goodfellow, Courville, A., and Bengio, Y. ,2013. Maxout networks. In ICML’2013.\n",
    "[7] Jarrett, K. and LeCun, Y. 2009, What is the best multi-stage architecture for object recognition, pp 2146–2153. IEEE.\n",
    "[8] Krizhevsky, A. and Hinton, G. 2009, Learning multiple layers of features from tiny images. Technical report, University of Toronto.\n",
    "[9] LeCun and Haffner, P. 1998, Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324.\n",
    "[10] Rifai, S. and Vincent, P. 2012, A generative process for sampling contractive auto-encoders. In ICML’12.\n",
    "[11] Susskind, J and Hinton, G. E. ,2010, The Toronto face dataset. Technical Report UTML TR 2010-001, U. Toronto.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
